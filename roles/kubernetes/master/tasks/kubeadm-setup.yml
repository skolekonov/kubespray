---
- name: kubeadm | Check if old apiserver cert exists on host
  stat:
    path: "{{ kube_cert_dir }}/apiserver.pem"
  register: old_apiserver_cert
  delegate_to: "{{groups['kube-master']|first}}"
  run_once: true

- name: kubeadm | Migrate old certs if necessary
  import_tasks: kubeadm-migrate-certs.yml
  when: old_apiserver_cert.stat.exists

- name: kubeadm | Check serviceaccount key
  stat:
    path: "{{ kube_cert_dir }}/apiserver.key"
  register: sa_key_before
  run_once: true

- name: kubeadm | Check if kubeadm has already run
  stat:
    path: "/var/lib/kubelet/config.yaml"
  register: kubeadm_already_run

- name: kubeadm | Delete old admin.conf
  file:
    path: "{{ kube_config_dir }}/admin.conf"
    state: absent
  when:
    - not kubeadm_already_run.stat.exists

- name: kubeadm | Delete old static pods
  file:
    path: "{{ kube_config_dir }}/manifests/{{item}}.manifest"
    state: absent
  with_items: ["kube-apiserver", "kube-controller-manager", "kube-scheduler", "kube-proxy"]
  when:
    - old_apiserver_cert.stat.exists

- name: kubeadm | Forcefully delete old static pods
  shell: "docker ps -f name=k8s_{{item}} -q | xargs --no-run-if-empty docker rm -f"
  with_items: ["kube-apiserver", "kube-controller-manager", "kube-scheduler"]
  when:
    - old_apiserver_cert.stat.exists

- name: kubeadm | aggregate all SANs
  set_fact:
    apiserver_sans: >-
      kubernetes
      kubernetes.default
      kubernetes.default.svc
      kubernetes.default.svc.{{ dns_domain }}
      {{ kube_apiserver_ip }}
      localhost
      127.0.0.1
      {{ ' '.join(groups['kube-master']) }}
      {%- if loadbalancer_apiserver is defined %}
      {{ apiserver_loadbalancer_domain_name }}
      {% endif %}
      {% for host in groups['kube-master'] -%}
      {%- if hostvars[host]['access_ip'] is defined %}
      {{ hostvars[host]['access_ip'] }}
      {% endif %}
      {{ hostvars[host]['ip'] | default(fallback_ips[host]) }}
      {%- endfor %}
      {% if supplementary_addresses_in_ssl_keys is defined -%}
      {% for addr in supplementary_addresses_in_ssl_keys %}
      {{ addr }}
      {% endfor %}
      {%- endif %}
  tags: facts

- name: Create audit-policy directory
  file:
    path: "{{ audit_policy_file | dirname }}"
    state: directory
  when: kubernetes_audit|default(false)

- name: Write api audit policy yaml
  template:
    src: apiserver-audit-policy.yaml.j2
    dest: "{{ audit_policy_file }}"
  when: kubernetes_audit|default(false)

# Nginx LB(default), If kubeadm_config_api_fqdn is defined, use other LB by kubeadm controlPlaneEndpoint.
- name: set kubeadm_config_api_fqdn define
  set_fact:
    kubeadm_config_api_fqdn: "{{ apiserver_loadbalancer_domain_name|default('lb-apiserver.kubernetes.local') }}"
  when: loadbalancer_apiserver is defined

- name: kubeadm | set kubeadm version
  import_tasks: kubeadm-version.yml

- name: kubeadm | Certificate management with kubeadm
  import_tasks: kubeadm-certificate.yml
  when:
    - not upgrade_cluster_setup
    - kubeadm_already_run.stat.exists

- name: kubeadm | Initialize first master
  command: >-
    timeout -k 600s 600s
    {{ bin_dir }}/kubeadm init
    --config={{ kube_config_dir }}/kubeadm-config.yaml
    --ignore-preflight-errors=all
    {% if kubeadm_version is version('v1.14.0', '>=') %}
    --experimental-upload-certs
    {% endif %}
    --skip-phases=addon/coredns
    {% if kubeadm_certificate_key is defined %}
    --certificate-key={{ kubeadm_certificate_key }}
    {% endif %}
  register: kubeadm_init
  # Retry is because upload config sometimes fails
  retries: 3
  when: inventory_hostname == groups['kube-master']|first and not kubeadm_already_run.stat.exists
  failed_when: kubeadm_init.rc != 0 and "field is immutable" not in kubeadm_init.stderr
  environment:
    PATH: "{{ bin_dir }}:{{ ansible_env.PATH }}"
  notify: Master | restart kubelet

- name: set kubeadm certificate key
  set_fact:
    kubeadm_certificate_key: "{{ item | regex_search('--certificate-key ([^ ]+)','\\1') | first }}"
  with_items: "{{ (kubeadm_init_output|default({'stdout_lines': []}))['stdout_lines'] }}"
  run_once: yes
  when:
    - kubeadm_version is version('v1.14.0', '>=')
    - kubeadm_certificate_key is not defined
    - kubeadm_init is defined
    - item | trim | match('.*--certificate-key .*')

- name: Create kubeadm token for joining nodes with 24h expiration (default)
  command: "{{ bin_dir }}/kubeadm --kubeconfig /etc/kubernetes/admin.conf token create"
  run_once: true
  register: temp_token
  delegate_to: "{{ groups['kube-master'][0] }}"
  when: kubeadm_token is not defined
  tags:
    - kubeadm_token

- name: Set kubeadm_token
  set_fact:
    kubeadm_token: "{{ temp_token.stdout }}"
  when: kubeadm_token is not defined
  tags:
    - kubeadm_token

- name: Create hardcoded kubeadm token for joining nodes with 24h expiration (if defined)
  shell: >-
    {{ bin_dir }}/kubeadm --kubeconfig /etc/kubernetes/admin.conf token delete {{ kubeadm_token }} || :;
    {{ bin_dir }}/kubeadm --kubeconfig /etc/kubernetes/admin.conf token create {{ kubeadm_token }}
  run_once: true
  when:
    - inventory_hostname == groups['kube-master']|first
    - kubeadm_token is defined
  tags:
    - kubeadm_token

- name: kubeadm | Initialize other masters (experimental control plane)
  include: kubeadm-secondary-experimental.yml
  when: kubeadm_control_plane

- name: kubeadm | Initialize other masters (experimental control plane)
  include: kubeadm-secondary-legacy.yml
  when: not kubeadm_control_plane

- name: kubeadm | upgrade kubernetes cluster
  import_tasks: kubeadm-upgrade.yml
  when: upgrade_cluster_setup

- name: kubeadm | Check serviceaccount key again
  stat:
    path: "{{ kube_cert_dir }}/sa.key"
  register: sa_key_after
  run_once: true

- name: kubeadm | Set secret_changed if service account key was updated
  command: /bin/true
  notify: Master | set secret_changed
  when: sa_key_before.stat.checksum|default("") != sa_key_after.stat.checksum

- name: kubeadm | cleanup old certs if necessary
  import_tasks: kubeadm-cleanup-old-certs.yml
  when:
    - old_apiserver_cert.stat.exists

# FIXME(mattymo): from docs: If you don't want to taint your control-plane node, set this field to an empty slice, i.e. `taints: {}` in the YAML file.
- name: kubeadm | Remove taint for master with node role
  command: "{{ bin_dir }}/kubectl --kubeconfig {{ kube_config_dir }}/admin.conf taint node {{ inventory_hostname }} node-role.kubernetes.io/master:NoSchedule-"
  delegate_to: "{{groups['kube-master']|first}}"
  when: inventory_hostname in groups['kube-node']
  failed_when: false
